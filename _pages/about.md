---
permalink: /
title:
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I recently completed my Ph.D. in Electrical and Computer Engineering (ECE) at the [University of Toronto](https://www.utoronto.ca/) (UofT), advised by Prof. [Stark C. Draper](https://www.ece.utoronto.ca/people/draper-s/), working at the intersection of distributed systems, machine learning, and data privacy. Before that, I received my MASc. in ECE at UofT, and B.Sc. in Electrical Engineering with a minor in Economics at the [Sharif University of Technology](http://www.en.sharif.edu/).

To expand industry and academic collaborations, I conducted internships at [CISPA Helmholtz Center for Information Security](https://sprintml.com/), Huawei's Accelerated Neural Technology team, and the Chinese University of Hong Kong.
I've also been certified at the [International High-Performance Computing Summer School, Japan (IHPCSS'2019)](https://ss19.ihpcss.org/) and the [North American School of Information Theory, USA (NASIT'2023)](https://nasit.seas.upenn.edu/). 

**I am currently on the 2025 job market. Please contact me if you would like to discuss potential openings or collaborations!**

## Research Interests

---

I enjoy creating novel solutions for big data and machine learning problems while exploring diverse research topics, transferring ideas across domains, and ensuring practical effectiveness. My research has focused on distributed systems with heterogeneous characteristics—such as varying computational speeds, privacy constraints, and utility objectives. In such settings, I have worked on **distributed matrix multiplication**, a fundamental operation in ML training and inference, and **federated learning**, which enables collaborative model training while keeping data local. To address stragglers in distributed computation and enhance privacy in federated learning, I have applied ideas from:

* **Error-correction codes** add efficient redundant computation through coding and recover results through decoding.

* **Randomized, numerically stable, and approximate algorithms** trade off accuracy for faster task recovery using polynomial interpolation.

* **Differential privacy** anonymizes data participation by applying controlled perturbation through noise, sampling, and clipping.

* **Optimization** tunes algorithm parameters and ensures training convergence under privacy and utility constraints.

I implemented our differentially private federated learning solutions using Python with PyTorch and Opacus, and evaluated our straggler-resilient methods on Amazon EC2 and Canada’s SciNet HPC cluster.

## Personal

---

Outside of research, I enjoy practicing creativity through cooking and experimenting with new dishes, hiking new trails, and trying to win at backgammon or other strategic games. When I'm not feeling lazy, I spend time gardening. Whenever possible, I travel to explore new cultures.



## Recent News

---

* March 2025: I passed my Ph.D. Final Oral Exam and submitted my Ph.D. thesis, which was accepted *as is* (with no required revisions)! Huge thanks to my committee members Prof. Stark Draper, Prof. Mohammad Ali Maddah-Ali, Prof. Azarakhsh Malekian, Prof. Dimitrios Hatzinakos, and Prof. Nicolas Papernot!

* February 2025: I was honored to receive the Farid and Diana Najm Graduate Fellowship and Shahid U. H. Qureshi Memorial Scholarship (both awarded to one ECE student per year), as well as the PhD Graduate Award for the second time! Many thanks to the donors for their support and trust in my work!

* January 2025: Our paper, [Differentially Private Federated Learning with Time-Adaptive Privacy Spending](https://openreview.net/forum?id=W0nydevOlG&noteId=zEslc0ErHW), has been accepted to ICLR 2025!

* December 2024: I passed my Ph.D. Departmental Oral Exam! Huge thanks to my committee members Prof. Stark Draper, Prof. Nicolas Papernot, and Prof. Dimitrios Hatzinakos.